# RunanywhereAI SDK Integration Guide for EduVerse

This guide documents the integration of the RunanywhereAI SDK into the EduVerse Android application,
enabling on-device AI chat capabilities.

## üìã What Was Integrated

### 1. SDK Dependencies

- **RunAnywhere Core SDK**: v0.1.3-alpha (via JitPack)
- **LlamaCpp Module**: Includes 7 optimized ARM64 CPU variants for on-device inference
- **Kotlin Coroutines**: For asynchronous operations

### 2. Configuration Changes

#### `settings.gradle.kts`

- Added JitPack repository for SDK distribution

#### `app/build.gradle.kts`

- Added RunanywhereAI SDK dependencies
- Upgraded Java version from 11 to 17 (required by SDK)
- Lowered minSdk from 33 to 24 for broader device support
- Added Kotlin Coroutines and ViewModel dependencies

#### `AndroidManifest.xml`

- Added required permissions: `INTERNET`, `WRITE_EXTERNAL_STORAGE`
- Configured custom Application class
- Enabled `largeHeap` for better model loading

### 3. New Files Created

#### `EduVerseApplication.kt`

Custom Application class that:

- Initializes the RunanywhereAI SDK on app startup
- Registers LlamaCpp service provider
- Registers 3 AI models (ranging from 119 MB to 815 MB)
- Scans for previously downloaded models
- Handles initialization errors gracefully

#### `MainActivity.kt` (Enhanced)

Full-featured AI chat application with:

- **Model Selector Screen**: Browse, download, and load AI models
- **Chat Screen**: Real-time streaming chat with the loaded model
- **ViewModel Architecture**: Proper state management with Kotlin Flow
- **Modern UI**: Material 3 design with Jetpack Compose

## üöÄ Next Steps to Complete Integration

### Step 1: Sync Gradle

1. Open Android Studio
2. Click **File > Sync Project with Gradle Files**
3. Wait for JitPack to build the SDK (first sync takes 2-3 minutes)
4. Subsequent syncs will be instant

### Step 2: Build and Run

1. Connect your Android device or start an emulator
2. Click **Run > Run 'app'**
3. The app will launch with the Model Selector screen

### Step 3: Test the Integration

#### Download a Model

1. On the Model Selector screen, you'll see 3 available models:
    - **SmolLM2 360M Q8_0** (119 MB) - Fastest, best for testing
    - **Qwen 2.5 0.5B Instruct Q6_K** (374 MB) - Good balance
    - **Llama 3.2 1B Instruct Q6_K** (815 MB) - Best quality

2. Click **Download** on any model
3. Watch the progress bar as the model downloads

#### Load and Chat

1. Once downloaded, click **Load Model**
2. The app switches to the Chat screen
3. Type a message and hit Send
4. Watch the AI respond in real-time with streaming tokens!

## üì± App Features

### Model Management

- **Browse Models**: See all registered models
- **Download Progress**: Real-time download progress indicators
- **One-Click Loading**: Load models with a single tap
- **Model Switching**: Switch between models via the "Models" button

### Chat Interface

- **Streaming Responses**: Tokens appear in real-time as they're generated
- **Message History**: Scrollable conversation history
- **Auto-Scroll**: Automatically scrolls to new messages
- **Clear Chat**: Reset conversation with one tap
- **System Messages**: Helpful status updates from the app

### ViewModel Architecture

- **State Management**: Reactive UI with Kotlin StateFlow
- **Coroutine Scoping**: Proper lifecycle-aware async operations
- **Error Handling**: Graceful error messages displayed to user

## üîß How It Works

### Initialization Flow

```
App Launch
  ‚Üì
EduVerseApplication.onCreate()
  ‚Üì
SDK Initialization (background thread)
  ‚Üì
Register LlamaCpp Provider
  ‚Üì
Register 3 AI Models
  ‚Üì
Scan for Downloaded Models
  ‚Üì
Ready for Use
```

### Model Download & Load Flow

```
User selects model
  ‚Üì
Download with progress tracking (Kotlin Flow)
  ‚Üì
Model saved to app storage
  ‚Üì
User clicks "Load Model"
  ‚Üì
Model loaded into memory
  ‚Üì
Ready for inference
```

### Chat Flow

```
User types message
  ‚Üì
ViewModel.sendMessage()
  ‚Üì
RunAnywhere.generateStream(prompt)
  ‚Üì
Tokens emitted via Kotlin Flow
  ‚Üì
UI updates in real-time
  ‚Üì
Complete response displayed
```

## üìö Available Models

| Model | Size | Quality | Best For |
|-------|------|---------|----------|
| SmolLM2 360M Q8_0 | 119 MB | Basic | Testing, quick responses |
| Qwen 2.5 0.5B Instruct Q6_K | 374 MB | Good | General conversations |
| Llama 3.2 1B Instruct Q6_K | 815 MB | Best | High-quality chat |

**Recommendation**: Start with SmolLM2 360M for testing, then upgrade to Qwen 2.5 0.5B for
production use.

## üé® Key Code Locations

### SDK Initialization

```kotlin
// File: app/src/main/java/com/example/eduverse/EduVerseApplication.kt

RunAnywhere.initialize(
    context = this@EduVerseApplication,
    apiKey = "dev",
    environment = SDKEnvironment.DEVELOPMENT
)

LlamaCppServiceProvider.register()
```

### Model Registration

```kotlin
// File: app/src/main/java/com/example/eduverse/EduVerseApplication.kt

addModelFromURL(
    url = "https://huggingface.co/...",
    name = "Model Name",
    type = "LLM"
)
```

### Model Download

```kotlin
// File: app/src/main/java/com/example/eduverse/MainActivity.kt

RunAnywhere.downloadModel(modelId).collect { progress ->
    // progress is Float from 0.0 to 1.0
    _downloadProgress.value = modelId to progress
}
```

### Streaming Chat

```kotlin
// File: app/src/main/java/com/example/eduverse/MainActivity.kt

RunAnywhere.generateStream(prompt).collect { token ->
    // token is String (single token from model)
    assistantResponse += token
    // Update UI with new token
}
```

## üêõ Troubleshooting

### Gradle Sync Fails

**Problem**: JitPack timeout or build failure
**Solution**:

- Wait and try again (first build can take 2-3 minutes)
- Check internet connection
- Clear Gradle cache: `./gradlew clean`

### Model Download Fails

**Problem**: Download interrupted or fails
**Solutions**:

- Check internet connection
- Verify `INTERNET` permission in manifest
- Check available storage space
- Try a smaller model first (SmolLM2 360M)

### App Crashes When Loading Model

**Problem**: Out of memory error
**Solutions**:

- Ensure `android:largeHeap="true"` in manifest
- Close other apps to free memory
- Try a smaller model
- Test on a device with more RAM

### Model Loads But Generation is Slow

**Problem**: Slow token generation (normal on low-end devices)
**Solutions**:

- Try a smaller model (SmolLM2 360M is fastest)
- Reduce `maxTokens` in generation options
- Test on a device with better CPU
- This is normal for on-device inference

### Linter Errors in Android Studio

**Problem**: Red underlines in code before Gradle sync
**Solution**:

- Click **File > Sync Project with Gradle Files**
- Wait for sync to complete
- Errors should disappear after successful sync

## üîê Privacy & Security

### On-Device Processing

- All AI inference happens locally on the device
- No data sent to external servers
- Complete privacy for sensitive conversations
- Works offline after model download

### Model Storage

- Models stored in app-private directory
- Automatic integrity checking (SHA-256)
- Persistent across app restarts
- Can be deleted by clearing app data

## üìä Performance Metrics

### Model Sizes & Memory

- **SmolLM2 360M**: ~500 MB RAM when loaded
- **Qwen 2.5 0.5B**: ~800 MB RAM when loaded
- **Llama 3.2 1B**: ~1.5 GB RAM when loaded

### Generation Speed

- Depends on device CPU and model size
- SmolLM2 360M: ~10-30 tokens/sec on modern devices
- Larger models: ~5-15 tokens/sec on modern devices
- Streaming provides immediate feedback to user

## üöÄ Future Enhancements

### Potential Features to Add

1. **Persistent Chat History**: Save conversations to database
2. **Custom System Prompts**: Configure AI behavior
3. **Model Presets**: Quick-switch between model configs
4. **Conversation Export**: Share or save chat transcripts
5. **Voice Input**: Speech-to-text for hands-free interaction
6. **Multiple Conversations**: Separate chat threads
7. **Model Quantization**: Support for different quantization levels
8. **Cloud Fallback**: Hybrid on-device + cloud routing

### Advanced SDK Features (Not Yet Implemented)

- **Structured Outputs**: Type-safe JSON generation
- **Thinking Models**: Models with `<think>` tags
- **Custom Model Registration**: Add any GGUF model
- **Analytics Events**: Track SDK performance metrics
- **Memory Management**: Automatic model unloading

## üìñ Additional Resources

### Official Documentation

- [RunanywhereAI GitHub](https://github.com/RunanywhereAI/runanywhere-sdks)
- [Android SDK Quick Start](https://github.com/RunanywhereAI/runanywhere-sdks/blob/main/QUICKSTART_ANDROID.md)
- [SDK Architecture](https://github.com/RunanywhereAI/runanywhere-sdks/blob/main/sdk/runanywhere-kotlin/docs/KOTLIN_SDK_DOCUMENTATION.md)

### Community & Support

- Discord: [Join Community](https://discord.gg/pxRkYmWh)
- Email: founders@runanywhere.ai
- GitHub Issues: [Report Bugs](https://github.com/RunanywhereAI/runanywhere-sdks/issues)

## ‚úÖ Integration Checklist

- [x] Added JitPack repository to `settings.gradle.kts`
- [x] Added SDK dependencies to `app/build.gradle.kts`
- [x] Updated Java version to 17
- [x] Lowered minSdk to 24 for broader compatibility
- [x] Added required permissions to `AndroidManifest.xml`
- [x] Created `EduVerseApplication.kt` with SDK initialization
- [x] Registered AI models (SmolLM2, Qwen, Llama)
- [x] Created full-featured chat UI in `MainActivity.kt`
- [x] Implemented ViewModel with state management
- [x] Added model selector with download progress
- [x] Implemented streaming chat with real-time tokens
- [ ] **TODO**: Sync Gradle and test on device
- [ ] **TODO**: Download and test a model
- [ ] **TODO**: Verify chat functionality works

## üéâ You're Ready!

The RunanywhereAI SDK is now fully integrated into EduVerse. Simply:

1. Sync Gradle
2. Build and run the app
3. Download a model
4. Start chatting with on-device AI!

Enjoy your privacy-first, on-device AI-powered educational app! üöÄ
